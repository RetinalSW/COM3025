{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome\n",
        "In these labs we will be using a the programming language of Python and some of the core packages and frameworks most commonly used in scientific computing such as numpy, tensorflow, scikit packages and keras.\n",
        "\n",
        "**If you require a short introduction to Python please make sure to visit the following tutorials.**\n",
        "\n",
        "Introduction to Python: https://www.w3schools.com/python/python_intro.asp\n",
        "\n",
        "Introduction to Numpy: http://cs231n.github.io/python-numpy-tutorial/\n",
        "\n",
        "After you are familiar with Python and its syntax you can continue this lab that will introduce you to Jupyter Notebooks and the Keras framework in more detail."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Jupyter Notebooks"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using Jupyter Notebooks for the ease of showcasing and developing our solutions. As this is a web solution you will find it easy to use and easy to save, export and import your own solutions to the exercises. \n",
        "\n",
        "To help you save some time here are some of the keyboard shortcuts that you may find will help you speed up your work when using Jupyter Notebooks\n",
        "- **shift + enter** run cell, select below\n",
        "- **ctrl + enter** run cell\n",
        "- **A** insert cell above\n",
        "- **B** insert cell below\n",
        "- **C** copy cell\n",
        "- **V** paste cell\n",
        "- **D** delete selected cell\n",
        "- **shift + M** merge selected cells\n",
        "- **I** interrupt kernel\n",
        "- **0** restart kernel (with dialog)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resource handling\n",
        "\n",
        "Every time you open an `ipnyb` (a iPython Notebook) a corresponding python Kernel will be launched. This, over time and when using more complex models in the future, can resolve in issues with the computer resources.\n",
        "\n",
        "The more notebooks you open, the more kernels will get launched and you may find yourselves in a situation where you are getting `our of memory` error codes from the console. To make sure you are only using the memory for the active notebooks you may want to make sure to `shutdown` the kernels that you are not using.\n",
        "\n",
        "![Kernels](data/kernels.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Keras\n",
        "\n",
        "In these demonstrations we will be using Keras framework, a modern AI framework that is built on top of the older Tensorflow. Keras allows us to demonstrate concepts in a more efficient way without losing too much control.\n",
        "\n",
        "You can read the official documentation and explore the details that we will not have time to cover in these sessions on the following URL: https://keras.io/"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a sequential model\n",
        "\n",
        "Models can be created in both sequential and non-sqeuantial manner. For the start we will begin with the sequential model and how to create one.\n",
        "\n",
        "One of the first things that we would like you to do is to experience the ease with which you can create a simple sequential model. Following the official documentation explore the code below to familiarise yourselves."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "model = Sequential()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the relevant parts from Keras such as models and layers. We will explain these in future demonstrations.\n",
        "\n",
        "With a single line we have specified that the new model will be sequential model. The next thing we need to do is add the first `layer` and to specify the shape of the input."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Dense(32, input_dim=100))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer of the network needs to get the input shape information. This will be the input tensor that our model will process and will be processed on a layer to layer basis.\n",
        "\n",
        "After adding the first layer, we will specify the Activation layer with the following simple command."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can train the model we need to configure the learning process. The method to use is the `compile` method. In here we specify the `optimizer`, the `loss` function and the `metrics`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our introductory example we will focus on a 2 class - **binary** - classification. \n",
        "The model will train on Numpy arrays, so we will generate some dummy data that our network can \"train on\".\n",
        "\n",
        "For this purpose we need to import numpy with the `import <module> as <alias>` syntax."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(2, size=(1000, 1))\n",
        "\n",
        "np.max(data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have generated random data with two possible labels (remember we are working on binary classification only) we are ready to start the training of our network."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(data, labels, epochs=10, batch_size=32)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the output above you can see the verbose output of the training process. \n",
        "\n",
        "We can observe:\n",
        "- How long it took to finish each epoch\n",
        "- What was the loss function value\n",
        "- What was the accuracy "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "- Experiment with the number of `epochs` and the `batch_size` and examine the training time, loss function and the accuracy of your created model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model just as explained above"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Properties of the Model\n",
        "\n",
        "We can inspect the properties of the model by the use of a handy function `.summary()` that we can call from our model as you can see below."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a more complex Model\n",
        "\n",
        "It is very easy to add layers to our model in Keras with the simple call of the `add()` method. In the example below we create a more \"complex\" (for lack of a better word) model that will be used for our binary classification problem."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# For a binary classification problem\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(data, labels, epochs=50, batch_size=32)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualising our Model\n",
        "\n",
        "To better understand our own or any model design we can visualise the layers directly in Jupyter Notebooks. This requires additional libraries as you can see in the `import` below. These are not relevant to the contents of this module so we will be using them `as-is`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try to make it better"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: \n",
        "\n",
        "With the knowledge you have aquired try to complete the following tasks:\n",
        "- Create a new model with multiple layers\n",
        "- Display the summary of the model properties\n",
        "- Try to adjust the number of epochs and batch size to improve the model accuracy\n",
        "- As an extra task you can generate additional random dataset with more training data\n",
        "- Visualise the model you have created"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Generate more data\n",
        "# Create new model\n",
        "# Display the summary of the model\n",
        "# Train\n",
        "# Visualise the model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI 'Hello World' - Loading MNIST dataset\n",
        "One of the most important (if not THE most important) things in Machine Learning is the dataset you are using to solve a particular problem. It has become a de facto \"hello world\" of AI and ML to use the MNIST dataset.\n",
        "\n",
        "What is MNIST? It is a dataset of hand-written digits in grayscale with the resolution of 28 by 28 pixels. The dataset consists of 60 000 images and 10 000 images in the test set. The way you split your dataset is very important as discussed later in this course."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luckily because MNIST is such a basic and overused dataset, Keras has a simple way to load the dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the dataset is separated to Train and Test groups.\n",
        "The `mnist.load_data()` returns 2 tuples:\n",
        "\n",
        "- x_train, x_test: uint8 array of grayscale image data with shape (num_samples, 28, 28).\n",
        "- y_train, y_test: uint8 array of digit labels (integers in range 0-9) with shape (num_samples,)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have discussed shape before. We can look at the shape of the data we have loaded by simply calling `.shape` on a single element from the dataset. This is a good debugging technique when working on more complex solutions."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0].shape\n",
        "x_train.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "So how does the \"real\" data of a single digit look like? Simply print out to the screen a single element from the train dataset as follows."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualise any element from the dataset using `matplotlib`, yet another library that is very handy in python."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import random as rng\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imshow(x_train[rng.randint(0,9)], cmap='gray')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "- Print out the shape of a certain element from the training set and also its array as you did before\n",
        "- Visualise, using `matplotlib` the digit that you displayed"
      ],
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data encoding\n",
        "\n",
        "## Categorical data -> Numerical data\n",
        "\n",
        "In many cases, our data is not represented as numerical data, but rather a set of items (categories). \n",
        "\n",
        "Example of numerical data:\n",
        "- `weight, price, score`\n",
        "\n",
        "Example of categorical data:\n",
        "- `pet [\"dog\", \"cat\", \"parrot\"]`\n",
        "- `place [\"London\", \"Ireland\", \"Guildford\"]`\n",
        "\n",
        "Because neural networks and many other algorithms cannot handle direct text/categorical input, we need a way to represent categorical data as numerical data. \n",
        "\n",
        "### Integer Encoding \n",
        "We assign each element a number based on `index` in `array`.\n",
        "\n",
        "`place [\"London\", \"Ireland\", \"France\", \"Italy\", \"Guildford\"]`  \n",
        "`London = 0, Ireland = 1, France = 2, Italy = 3, Guildford = 4`\n",
        "\n",
        "This method can lead to **issues**, because we cannot apply normal mathematical operations.  \n",
        "`4/2 = 2` but equation `Guildford/France = France` does not make any sense. \n",
        "\n",
        "### One Hot Encoding\n",
        "In this method we create array of zeros with size of all possible options and set index of element we chose to 1. \n",
        "\n",
        "Example:  \n",
        "`place [\"London\", \"Ireland\", \"France\", \"Italy\", \"Guildford\"]\n",
        "London =    [1,0,0,0,0]\n",
        "Ireland =   [0,1,0,0,0]\n",
        "France =    [0,0,1,0,0]\n",
        "Italy =     [0,0,0,1,0]\n",
        "Guildford = [0,0,0,0,1]`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way we can represent each \"category\" or \"class\" as a single 1 in an array of 0s.\n",
        "\n",
        "Even though many frameworks include methods to do this for us, for the sake of clarification we will implement our own version of one hot encoding."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_one_hot(num_of_options, hot_index):\n",
        "    \"\"\"Utils function for creating one hot array (array of zeros with only one value set to 1)\n",
        "    \n",
        "    later on, we can use to_categorical function from keras library (from keras.utils import to_categorical)\n",
        "    instead of this function\n",
        "    \n",
        "    Arguments:\n",
        "        num_of_options {int} -- Number of possible items in the array\n",
        "        hot_index {int} -- Index of element that should be set to 1\n",
        "    \n",
        "    Raises:\n",
        "        ValueError -- Hot index exceeds the length of overall array\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray -- One_hot array\n",
        "    \"\"\"\n",
        "\n",
        "    if hot_index >= num_of_options:\n",
        "        raise ValueError(\"Hot index exceeds the length of overall array\")\n",
        "    \n",
        "    array = np.zeros((num_of_options))\n",
        "    array[hot_index] = 1\n",
        "    return array\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "all_posibilities = [\"cat\", \"dog\", \"elephant\", \"human\"]\n",
        "\n",
        "selected_element = \"elephant\" #try changing this to any element from all_posibilities\n",
        "selected_index = all_posibilities.index(selected_element) #Find the index of our selected element\n",
        "print(selected_index)\n",
        "\n",
        "to_one_hot(len(all_posibilities),selected_index) #Use the method created above to create one_hot representation"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU or CPU?\n",
        "\n",
        "When using Tensorflow or Keras, it is advisable to use a dedicated GPU (unless you have too much free time) and for this reason it is good practice before doing any kind of training to check that your local installation is detecting your GPU.\n",
        "\n",
        "If you do not have a dedicated GPU you may want to consider creating solutions for less complex problems and adjusting your training parameters."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if we can access GPU\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Classifier\n",
        "\n",
        "We have already created a binary classifier in the earlier section of this demonstration. \n",
        "This time we will be re-creating it for the purposes of classifying a number from the MNIST dataset.\n",
        "\n",
        "First, we need to make sure that our dataset has the correct shape. As discussed earlier the shape of our pictures is `28 by 28` pixels."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape((60000, 28*28))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_1 = (y_train == 1)\n",
        "y_test_1 = (y_test == 1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implement **Linear Classifier** with Stochastic Gradient Descent (SGD) learning: *the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).*\n",
        "\n",
        "For more information please make sure to read the documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(max_iter=5, tol=-np.infty, random_state=42)\n",
        "sgd_clf.fit(x_train, y_train_1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, visualising the prediction is a good way to get an idea what is actually happening *under the hood* of the trained model. \n",
        "\n",
        "With the knowledge you now have your next **Exercise** is to:\n",
        "- import matplotlib library\n",
        "- import the random library\n",
        "- make sure that matplotlib works inline (hint: `%matplotlib inline`)\n",
        "- select a random number from the whole dataset\n",
        "- use the model to `predict` if the random number is classified correctly and display the results using `print` method\n",
        "- display the number using matplotlib (hint: remember to `reshape()` the image"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import random as rng\n",
        "%matplotlib inline\n",
        "\n",
        "number = rng.randint(0,60000)\n",
        "print(sgd_clf.predict([x_train[number]]))\n",
        "\n",
        "number_to_display = x_train[number].reshape((28, 28))\n",
        "plt.imshow(number_to_display, cmap='gray')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "- Change the binary classifier and train it to detect a different digit\n",
        "\n",
        "**Bonus Exercise**:\n",
        "- Can you create a classifier for multiple digits?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Validation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross validation using **K-fold cross-validation** splits at random the training set to `k` amount of distinct subsets. These are also known as **folds**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(sgd_clf, x_train, y_train_1, cv=10, scoring=\"accuracy\")\n",
        "rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "print(\"Scores: \", scores)\n",
        "print(\"Mean: \", scores.mean())\n",
        "print(\"Standard deviation: \", scores.std())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output above we can see that our model is trained quite well as the standard deviation is pretty low. If the standard deviation would be too high we may have an issue with overfitting. We will discover these in more detail next week."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way how to measure performance of our classifier is to use Confusion Matrix. Image is worth thousand words so an illustration will be most helpful.  \n",
        "![Confusion Matrix](data/binary_confusion_matrix.png)\n",
        "\n",
        "We count the number of times our classifier has misclassified and we compare them to actual class of a set of predictions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "y_train_pred = cross_val_predict(sgd_clf, x_train, y_train_1, cv=10)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_train_1, y_train_pred)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Erexcise**: Based on the output of the confusion matric and the image illustration provided\n",
        "- Identify the number of TP, FN, FP and TN"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precision vs Recall\n",
        "\n",
        "Precision is measured witht the following equation:  \n",
        "`precision = TP / (TP + FP)`\n",
        "\n",
        "In combination with recall we can get our True Positive Rate (TPR). Recall is calculated as follows:  \n",
        "`recall = TP / (TP + FN)`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "precision_score(y_train_1, y_train_pred)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(y_train_1, y_train_pred)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can combine precision and recall to a `F1` score.  \n",
        "We calculate the F1 score with the following formula:  \n",
        "![F1 Score](data/f1.png)\n",
        "\n",
        "Of course Scikit-Learn can help is in calculating this value."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_train_1, y_train_pred)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Always keep in mind the **precision/recall tradeoff** - Increasing precision reduces recall and vice versa.\n",
        "\n",
        "This can be illustrated using the following plot."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_scores = cross_val_predict(sgd_clf, x_train, y_train_1, cv=3,\n",
        "                             method=\"decision_function\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_1, y_scores)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
        "    plt.xlabel(\"Threshold\", fontsize=12)\n",
        "    plt.legend(loc=\"upper left\", fontsize=12)\n",
        "    plt.ylim([0, 1])\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "plt.xlim([-600000, 600000])\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_precision_vs_recall(precisions, recalls):\n",
        "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
        "    plt.xlabel(\"Recall\", fontsize=12)\n",
        "    plt.ylabel(\"Precision\", fontsize=12)\n",
        "    plt.axis([0, 1, 0, 1])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plot_precision_vs_recall(precisions, recalls)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:  \n",
        "- At roughly what precision rate does the recall fall most significantly?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Receiver Operating Characteristic (ROC) curve\n",
        "\n",
        "This is yet another tool used with binary classifiers. The difference is that unlike precision vs recall the ROC plots a curve of `True Positive Rate against the False Positive Rate (FPR)`.\n",
        "\n",
        "**FPR**: The ratio of negative instances that are incorrectly classified as positive."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first need to compute the TPR and FPR using the `roc_curve()` method"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_train_1, y_scores)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can plot FPR against TPR using our old friend - MatPlotLib"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(fpr, tpr, linewidth=2, label=None)\n",
        "plt.plot([0,1], [0,1], 'k--')\n",
        "plt.axis([0,1,0,1])\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare classifiers we can measure the **area under the curve (AUC)**.  \n",
        "An ideal classifier will have a ROC AUC == 1, and a completely random one will have the value == 0.5  \n",
        "\n",
        "We can compute ROC AUC using Scikit-Learn pretty easily, as follows:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(y_train_1, y_scores)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilabel Classification"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to predict more than two classes (binary classification) we need to train multiclass classifiers (multinominal classifiers).\n",
        "\n",
        "Since Support Vector Machine or Linear classifiers are binary classifiers, we have to perform multiclass classification using multiple binary classifiers."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 28*28)\n",
        "sgd_clf.fit(x_train, y_train)\n",
        "sgd_clf.predict([x_train[1601]])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:  \n",
        "- Confirm that the prediction is correct by visualising the predicted number using matplotlib"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilabel Classification\n",
        "\n",
        "If we need to recognise multiple labels in one picture, for example three different people we create a multilabel classification system.\n",
        "\n",
        "A simple implementation follows"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "y_train_large = (y_train >= 7)\n",
        "y_train_odd = (y_train % 2 == 1)\n",
        "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
        "\n",
        "knn_clf = KNeighborsClassifier()\n",
        "knn_clf.fit(x_train, y_multilabel)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have created our `y_multilabel` array that contains two targets - Large numbers (greater than 7) and Odd numbers.\n",
        "\n",
        "### KNeighborsClassifier\n",
        "\n",
        "Next we create a `KNeighborsClassifier` instance and we train it on the multiple target array. The output should now produce two labels."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "knn_clf.predict([x_train[1200]])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "- Visualise the predicted image to verify the correctness of the prediction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will take a look at the performance of our multilabel classifier using the known `F1` score."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_knn_pred = cross_val_predict(knn_clf, x_train, y_multilabel, cv=3, n_jobs=-1)\n",
        "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Labs_Env)",
      "language": "python",
      "name": "labs_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "nteract": {
      "version": "0.21.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}